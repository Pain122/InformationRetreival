{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Words, concepts and search\n",
    "\n",
    "You are given a collection of texts -- let's call it dataset. Each string is a separate document. For each test we will answer exactly 2 questions:\n",
    "- Which of the documents (`doc_id`) is the closest to a given query?\n",
    "- How many \"concepts\" (`concept_count`) are enough to represent this dataset?\n",
    "\n",
    "Thus your result (answer) will consist of 2 integers, separated by a space. Like this: `doc_id concept_count`.\n",
    "\n",
    "## Let's consider the test example:\n",
    "\n",
    "`input.txt`\n",
    "\n",
    "```\n",
    "c d b.\n",
    "d e a.\n",
    "a b c.\n",
    "a b c d.\n",
    "d c a b.\n",
    "a c.      # <--- the last one is the query\n",
    "```\n",
    "\n",
    "## Let's do vectorization\n",
    "Reuse this in your solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['a', 'b', 'c', 'd', 'e']\n",
      "[[0.         0.57735027 0.57735027 0.57735027 0.        ]\n",
      " [0.440627   0.         0.         0.440627   0.78210977]\n",
      " [0.57735027 0.57735027 0.57735027 0.         0.        ]\n",
      " [0.5        0.5        0.5        0.5        0.        ]\n",
      " [0.5        0.5        0.5        0.5        0.        ]]\n",
      "\n",
      "Is it normed?\n",
      "\n",
      "[[1.         0.25439612 0.66666667 0.8660254  0.8660254 ]\n",
      " [0.25439612 1.         0.25439612 0.440627   0.440627  ]\n",
      " [0.66666667 0.25439612 1.         0.8660254  0.8660254 ]\n",
      " [0.8660254  0.440627   0.8660254  1.         1.        ]\n",
      " [0.8660254  0.440627   0.8660254  1.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "inp = \"\"\"c d b.\n",
    "d e a.\n",
    "a b c.\n",
    "a b c d.\n",
    "d c a b.\n",
    "a c.\"\"\".split('\\n')\n",
    "\n",
    "dataset, query = inp[:-1], inp[-1]\n",
    "\n",
    "vect = TfidfVectorizer(\n",
    "            analyzer='word',\n",
    "            stop_words=None,\n",
    "            token_pattern=r\"(?u)\\b\\w+\\b\"    # (?u)\\b\\w\\w+\\b -- default pattern: (?u) -- unicode modifier, \\b -- word border, \\w\\w+ = 2+ letters\n",
    ")\n",
    "DTM = vect.fit_transform(dataset).todense()\n",
    "print(\"Vocabulary:\", vect.get_feature_names())\n",
    "print(DTM)\n",
    "\n",
    "print(\"\\nIs it normed?\\n\")\n",
    "print(DTM @ DTM.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So, we are ready to answer question 1. \n",
    "Which of the documents is the closest to a given query?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarities of query and dataset:\n",
      " [[0.40824829]\n",
      " [0.31157034]\n",
      " [0.81649658]\n",
      " [0.70710678]\n",
      " [0.70710678]]\n",
      "Best matching document: 2\n"
     ]
    }
   ],
   "source": [
    "query_vector = vect.transform([query]).todense()\n",
    "cosines_raw = DTM @ query_vector.T # ... oops. Your code here. Compute COSINE SIMILARITIES (1 is match, 0 - orthogonal)\n",
    "print(\"Cosine similarities of query and dataset:\\n\", cosines_raw)\n",
    "# ... oops. Some more code here. Find an id of the best matching document (string number in test data)\n",
    "print(f\"Best matching document: {cosines_raw.argmax()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time for question 2.\n",
    "How many concepts are enough to express our dataset?\n",
    "\n",
    "In other words, how many orthogonal components do we need to pass `allclose` test?\n",
    "\n",
    "**NB: Can you just take the data and run PCA? Will it change the cosine metric?**\n",
    "\n",
    "Implement reduced SVD decomposition to pass the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, sigma, Vh = np.linalg.svd(DTM, full_matrices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.75035742e-16  5.77350269e-01  5.77350269e-01  5.77350269e-01\n",
      "  -2.90313134e-16]\n",
      " [ 4.40627001e-01 -1.22990992e-16 -4.10234178e-17  4.40627001e-01\n",
      "   7.82109769e-01]\n",
      " [ 5.77350269e-01  5.77350269e-01  5.77350269e-01  1.11369166e-17\n",
      "   2.44119866e-16]\n",
      " [ 5.00000000e-01  5.00000000e-01  5.00000000e-01  5.00000000e-01\n",
      "   9.33004473e-18]\n",
      " [ 5.00000000e-01  5.00000000e-01  5.00000000e-01  5.00000000e-01\n",
      "   3.47880079e-17]]\n",
      "(5, 5)\n",
      "If we take 1 components, allclose = False\n",
      "(5, 5)\n",
      "If we take 2 components, allclose = False\n",
      "(5, 5)\n",
      "If we take 3 components, allclose = False\n",
      "(5, 5)\n",
      "If we take 4 components, allclose = True\n",
      "(5, 5)\n",
      "If we take 5 components, allclose = True\n"
     ]
    }
   ],
   "source": [
    "from numpy.linalg import svd\n",
    "\n",
    "U, sigma, Vh = np.linalg.svd(DTM, full_matrices=True)\n",
    "Sigma = np.diag(sigma)\n",
    "print(U @ Sigma @ Vh)\n",
    "for i in range(1, min(DTM.shape[1], DTM.shape[0]) + 1):\n",
    "    doc_embeddings =  U[:, :i] @ Sigma[:i, :]\n",
    "    projection =  Vh[:, :]\n",
    "    DTM_approx =  doc_embeddings @ projection\n",
    "    print(f\"If we take {i} components, allclose =\", np.allclose(DTM, DTM_approx, atol=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarities of query and dataset (after reduction to 4 dimensions):\n",
      " [[0.40824829]\n",
      " [0.31157034]\n",
      " [0.81649658]\n",
      " [0.70710678]\n",
      " [0.70710678]]\n"
     ]
    }
   ],
   "source": [
    "k = 4\n",
    "\n",
    "sig = [sigma[j] if j < k else 0 for j in range(sigma.shape[0])]\n",
    "Sigma = np.diag(sig)\n",
    "doc_embeddings =  U @ Sigma \n",
    "projection = Vh\n",
    "query_embedding = projection @ query_vector.T\n",
    "\n",
    "cosines = doc_embeddings @ query_embedding\n",
    "print(f\"Cosine similarities of query and dataset (after reduction to {k} dimensions):\\n\", cosines)\n",
    "assert np.allclose(cosines, cosines_raw), \"Cosine similariries are not close\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exactly the same! We verified the job!\n",
    "\n",
    "### And the answer is...\n",
    "\n",
    "Thus, looking at the result of previous block, we can state:\n",
    "    \n",
    "`output.txt`\n",
    "```\n",
    "2 4\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
